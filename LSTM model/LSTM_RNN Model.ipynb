{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55781,"status":"ok","timestamp":1735882821640,"user":{"displayName":"Chris George","userId":"01709084539419508860"},"user_tz":-330},"id":"WjtnZf9gteFm","outputId":"98b13b7e-6d45-47dd-97f9-2285305e4552"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14459,"status":"ok","timestamp":1735882836094,"user":{"displayName":"Chris George","userId":"01709084539419508860"},"user_tz":-330},"id":"7FOkd_lZuVQ_","outputId":"709257b5-2eef-4c2c-a9c7-4cd157bc065e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import re\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","\n","# Download nltk data files (run this once)\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Initialize lemmatizer and stopwords\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","# Text preprocessing function\n","def preprocess_text(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Remove special characters, numbers, and punctuation\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Tokenize and remove stopwords, apply lemmatization\n","    tokens = text.split()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n","    return ' '.join(tokens)\n","\n","# Load and preprocess your dataset\n","data = pd.read_csv('/content/drive/MyDrive/CURA GPT/LSTM model/health_queries.csv')  # Ensure columns 'symptoms' and 'disease' exist\n","data['symptoms'] = data['symptoms'].apply(preprocess_text)  # Apply preprocessing to symptoms column\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":700,"status":"ok","timestamp":1735882836791,"user":{"displayName":"Chris George","userId":"01709084539419508860"},"user_tz":-330},"id":"V5_1ccz3upil"},"outputs":[],"source":["# Tokenize and pad sequences\n","tokenizer = Tokenizer(num_words=5000, oov_token=\"\u003cOOV\u003e\")\n","tokenizer.fit_on_texts(data['symptoms'])\n","sequences = tokenizer.texts_to_sequences(data['symptoms'])\n","max_len = max([len(x) for x in sequences])\n","padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n","\n","# One-hot encode the target labels\n","disease_labels = pd.get_dummies(data['disease']).values\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1735882836792,"user":{"displayName":"Chris George","userId":"01709084539419508860"},"user_tz":-330},"id":"n3KPBjEKuvy-"},"outputs":[],"source":["# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, disease_labels, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148730,"status":"ok","timestamp":1735882985514,"user":{"displayName":"Chris George","userId":"01709084539419508860"},"user_tz":-330},"id":"YCr0XNdauzl3","outputId":"b4ac1363-7188-4306-c055-0b8a2e31fe7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 111ms/step - accuracy: 0.0675 - loss: 3.3931 - val_accuracy: 0.3211 - val_loss: 2.0374\n","Epoch 2/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 86ms/step - accuracy: 0.4425 - loss: 1.7859 - val_accuracy: 0.6931 - val_loss: 1.1170\n","Epoch 3/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 85ms/step - accuracy: 0.7040 - loss: 1.0032 - val_accuracy: 0.8232 - val_loss: 0.6751\n","Epoch 4/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 63ms/step - accuracy: 0.8147 - loss: 0.6166 - val_accuracy: 0.8841 - val_loss: 0.3522\n","Epoch 5/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 72ms/step - accuracy: 0.8821 - loss: 0.3770 - val_accuracy: 0.9248 - val_loss: 0.3484\n","Epoch 6/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 86ms/step - accuracy: 0.9414 - loss: 0.2496 - val_accuracy: 0.9736 - val_loss: 0.0966\n","Epoch 7/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 71ms/step - accuracy: 0.9736 - loss: 0.0887 - val_accuracy: 0.9766 - val_loss: 0.0660\n","Epoch 8/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.9827 - loss: 0.0540 - val_accuracy: 1.0000 - val_loss: 0.0361\n","Epoch 9/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 76ms/step - accuracy: 0.9416 - loss: 0.2307 - val_accuracy: 0.9878 - val_loss: 0.1039\n","Epoch 10/10\n","\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 83ms/step - accuracy: 0.9953 - loss: 0.0463 - val_accuracy: 1.0000 - val_loss: 0.0160\n"]},{"data":{"text/plain":["\u003ckeras.src.callbacks.history.History at 0x7bbd1ed0fe80\u003e"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Build the LSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_len))\n","model.add(LSTM(128, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(64))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(disease_labels.shape[1], activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"VcIBudDzwZvZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello! I'm your medical assistant. Describe your symptoms, and I'll try to predict the disease.\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step\n","Chatbot: Based on your symptoms, you might have This could be a sign of Typhoid.. Please consult a doctor for a thorough diagnosis.\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","Chatbot: Based on your symptoms, you might have This could be a sign of Hyperthyroidism.. Please consult a doctor for a thorough diagnosis.\n"]}],"source":["# Function to predict disease based on user input\n","def predict_disease(symptoms_text):\n","    # Preprocess the input text\n","    processed_text = preprocess_text(symptoms_text)\n","    # Tokenize and pad the sequence\n","    seq = tokenizer.texts_to_sequences([processed_text])\n","    padded_seq = pad_sequences(seq, maxlen=max_len, padding='post')\n","    # Predict the disease\n","    prediction = model.predict(padded_seq)\n","    predicted_disease = np.argmax(prediction, axis=1)\n","    return data['disease'].unique()[predicted_disease[0]]\n","\n","# Chatbot interaction loop\n","print(\"Hello! I'm your medical assistant. Describe your symptoms, and I'll try to predict the disease.\")\n","while True:\n","    symptoms_input = input(\"You:\")\n","    if symptoms_input.lower() in ['quit', 'exit']:\n","        print(\"Goodbye! Take care.\")\n","        break\n","    disease_prediction = predict_disease(symptoms_input)\n","    print(f\"Chatbot: Based on your symptoms, you might have {disease_prediction}. Please consult a doctor for a thorough diagnosis.\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMgkvalzXCipgJncptGsgkJ","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}